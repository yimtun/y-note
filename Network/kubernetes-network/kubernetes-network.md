[toc]











[toc]



## 00 

overheads  å¼€é”€



## 01 2022-04-15

cilium

calico

flannel

ensp 

æ˜ç»†è·¯ç”±

BGP

OSPF

xdp

dpdk

ipvlan



## 02 2022-04-18







## 03 2022-04-19



###  k8s1.23.5







## 04 2022-04-20

ç½‘ç»œåŸºç¡€





## 05 2022-04-22





cni

ipvlan cni





overlay éš§é“

underlay  





ipvlan æ²¡æœ‰lay

macla æ²¡æœ‰lay

SR-IOV  x710

dpdk   pmd

vpp  



docker host gateway

```
docker network create -d bridge --subnet 172.19.0.0/16 br19

docker run --name d01 --network br19 -td xxximag


route add -net 172.19.0.0/16 gw 192.168.2.61

ä¸¤è¾¹éƒ½åŠ 


ä¸åšsnat æ€ä¹ˆå®ç°
```



docker vxlan





```
ç›´è¿è·¯ç”±
tcpdump -pne  -i eth0
-p å…³é—­æ··æ‚æ¨¡å¼
```



## 06 2022-04-25-pass





```
cilium  v1.10

https://docs.cilium.io/en/v1.10/gettingstarted/




CONFIG_BPF=y
CONFIG_BPF_SYSCALL=y
CONFIG_NET_CLS_BPF=y
CONFIG_BPF_JIT=y
CONFIG_NET_CLS_ACT=y
CONFIG_NET_SCH_INGRESS=y
CONFIG_CRYPTO_SHA1=y
CONFIG_CRYPTO_USER_API_HASH=y
CONFIG_CGROUPS=y
CONFIG_CGROUP_BPF=y




å®‰è£…æœ€æ–°ç‰ˆ   ä¸ä½¿ç”¨ cilium å‘½ä»¤è¡Œ æ–¹æ³•å®‰è£…

curl -L --remote-name-all \
https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum}

sha256sum --check cilium-linux-amd64.tar.gz.sha256sum

sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin

rm cilium-linux-amd64.tar.gz{,.sha256sum}

cilium status
cilium versin
cilium install

cilium status

get cilium configmap  é…ç½®å­˜æ”¾ä½ç½®


cilium uninstall

```







```
heml template

heml install --dry-run  --set name=cilium cilium ./tplate_dir > cilium.yml




heml add 
helm repo add
helm  repo update





Default Configuration:

Datapath	IPAM	Datastore
Encapsulation	Cluster Pool	Kubernetes CRD





https://docs.cilium.io/en/v1.10/gettingstarted/kubeproxy-free/#kubeproxy-free
```







```
https://docs.cilium.io/en/v1.10/gettingstarted/k8s-install-kubeadm/


```







```
helm template cilium cilium/cilium \
--set version=1.10.6 \
--set namespace=kube-system \
--set kubeProxyReplacement=strict\
--set k8sServiceHost=172.16.100.201\
--set k8sServicePort=6443 > cilium_1.10.6yaml
```



```
vim cilium_1.10.6yaml

debug true
debug-verbose: "datapath"


monitor-aggregaton: none

Encapsulation mode for communication between nodes
-disabled
-vxlan (default)
--geneve



```





```
debug true
debug-verbose: "datapath"
```





```
kubectl apply -f  cilium_1.10.6yaml
```





```
kubectl -n kube-system exec -it cilium-xxx -- bash

cilium status
KubeProxyReplacement: strict
Host Routingï¼š  Legacyï¼Ÿï¼Ÿ
Masquerading: IPTables ??
```





```
kubectl -n kube-system  logs  pod  | grep Falling 


requires enable-bpf-masquerade 
Falling back to enable-host-legacy-routing-true

```







```
vim cilium_1.10.6yaml


enable-bpf-masquerade: "true"

delete
kubectl apply -f xxx.yaml

kubectl -n kube-system logs cilium-xxx | grep subsys=daemon



cilium status
Host Routing BPF
Masquerading: BPF [ens33]

åˆ›å»ºä¸€ä¸ª deploy 
æš´éœ² service 
æ£€æŸ¥ æ²¡æœ‰å¯¹åº”çš„iptables å°±ç®—ok äº†

kube expose deploy 

iptables-save | grep svc_name 






```









extra



```
cilium status 

IPAM 3/254.  (å¯ç”¨åœ°å€1-254.  255å¹¿æ’­)
Cluster Pods  cilium æ¥ç®¡çš„pod


cat /etc/cni/net.d/05-cilium.conf

type: cilium-cni
```





```
https://docs.cilium.io/en/v1.10/gettingstarted/cni-chaining/
```







## 07 2022-04-27-pass





```
vxlan

mac in udp


ip mac


äºŒå±‚ mac
ä¸‰å±‚ ip
```









vxlan



https://support.huawei.com/enterprise/zh/doc/EDOC1100087027





```
stp
é€»è¾‘ä¸Šé˜»å¡ æŸä¸ªç«¯å£
stp å¯¼è‡´å¸¦å®½åˆ©ç”¨ç‡ é™ä½
```



```
mstp
vrrp
```





vrrp+smtp



http://www.h3c.com/cn/d_201305/785647_30003_0.htm







```
ecmp åè®®
vm  +  L2 +  leaf + L3 + spine (å»æ‰æ±‡èšå±‚ ä¸å†ä½¿ç”¨äºŒå±‚çš„é˜²ç¯æœºåˆ¶)
```







```
linux vxlan  ç‚¹å¯¹ç‚¹

ä¸¤ä¸ªæœåŠ¡å™¨ L3 å³å¯


vni



ip link add vxlan0 type vxlan id 5 dstport 4789 remote  192.168.2.62 local 192.168.2.61 dev ens33

ip -d link show vxlan0

vni id 5

4789  ç«¯å£æ˜¯udp ç«¯å£


ip addr add 20.1.1.2/24 dev vxlan0
ip link set vxlan0 up




æµ‹è¯•
ping -I x.x.x.x x.x.x.x

tcpdump -pne -i ens33 icmp
tcpdump -vvv -pne -i ens33 icmp


p å…³é—­æ··æ‚æ¨¡å¼  å¦åˆ™é™¤äº†è¿™ä¸¤ä¸ªæœåŠ¡å™¨çš„å…¶ä»–æœåŠ¡å™¨ä¸Š ä¹Ÿèƒ½æŠ“åˆ°åŒ…



æ¸…ç†arp

arp -d 10.0.1.1




```







```
vxlan group å¤šæ’­ç»„

æ²¡æœ‰local remote 


ip link add vxlan0 type vxlan id 6 dstport 4789 group 239.1.1.1 dev ens33

ip addr add 10.20.1.3/24 dev vxlan0

ip link set vxlan0 up

4789 ç«¯å£åˆ°åº•æ˜¯è°çš„


```







```
bgp è¾¹ç•Œç½‘å…³åè®®
bgp  rr    bgp è·¯ç”±åå°„å™¨
vtep 

bgp evpn peer è§£å†³ full mesh é—®é¢˜





vtep  æŠŠè‡ªå·±çš„ä¿¡æ¯ å‘Šè¯‰ bgp rr 
vtep ä» bgp rr è·å–å…¶ä»–vtepçš„ä¿¡æ¯





full mesh å…¨é“¾æ¥
æ¯ä¸ªvtepéƒ½è¿è·¯ç”±å™¨

```









evpn



https://support.huawei.com/enterprise/zh/doc/EDOC1100164807?idPath=24030814







https://arthurchiao.art/blog/spine-leaf-design-zh/













```
cilium


1.11
ipv4-native-routing-cidr: x.x.x.x/y
å¦‚æœä¸è®¾ç½®ä¼šåšsnat


1.10
native-routing-cidr: x.x.x.x/y



è¿›å…¥cilium å®¹å™¨ æŸ¥çœ‹æŸä¸ª vni id


cilium endpoint list

cilium identity list


lxc86xxxx  veth pair çš„ä¸€ç«¯  æŠ“åŒ… æœ‰å»æ— å›
å•å‘çš„

å›æ¥çš„åœ¨å“ªé‡Œ ï¼Ÿï¼Ÿ

```









## 08 2022-04-29-pass



```
host-routing   is XDPï¼ˆeXpress Data Pathï¼‰ xdp not kernel bypath
```



cilium  vxlan



overhead



https://cilium.io/blog/2021/05/11/cni-benchmark





https://cilium.io/blog/2020/11/10/cilium-19



```
eBPF host routing

bpf_redirect_peer   è¿›åˆ°podå†…éƒ¨
bpf_redirect_neigh  pod å‡ºæ¥
```



```
overhead
```





```
cilium datapath

```



```
two pod on same node

tcpdump -pne -i eth0  (in pod)

-p å…³é—­æ··æ‚æ¨¡å¼
tcpdump -pne -i (veth pair in root ns)
```





```
pod å†…éƒ¨ç½‘å¡ 32ä½æ©ç  ç›¸å½“äºæ²¡æœ‰å’Œå®ƒæ˜¯ä¸€ä¸ªç½‘æ®µçš„ï¼Œç›´æ¥èµ°äº†é»˜è®¤è·¯ç”± èµ°ä¸‰å±‚
è¿”å›çš„ä¸æ˜¯ç½‘å…³çš„mac åœ°å€ï¼Œï¼ˆè¿”å›çš„æ˜¯ veth pari in root ns  ç½‘å¡è®¾å¤‡çš„macï¼‰


arp æŸ¥è¯¢

åªæœ‰ request æ²¡æœ‰ reply
```







```
tc filter show ingress
tc filter show egress dev lxcssxwerr
```



```
kubectl -n kube-system exec -t cilium-xxx -- bash

cilium monitor 

cilium monitor -vv   >  xx.txt

identify=xxx

cilium identify list

pod-1 pod-2 on save node

Conntrack lookup  (in pod 1 å‘å‡ºå»çš„) 

Conntrack lookup  (in pod 2 æ”¶è¿›æ¥çš„)




åŒèŠ‚ç‚¹çš„pod æ‰ç”¨åˆ°  eBPF host routing


```











```
1  tcpdup
2  cilium monitor
3  iptables trace-id
4  pwru
```





```
pwru --filter-dst-ip x.x.x.x --filter-proto icmp --output-stack

pwru --filter-dst-ip x.x.x.x --filter-proto icmp --output-tuple
```



```
__ip_local_out
```



```
host reachable services
```





```
cilium 16è¿›åˆ¶
```



```
next cilium vxlan æ‰©èŠ‚ç‚¹
```







## 09 2022-05-01-pass





```
1 cilium vxlan pod  on  diffent node 
```









```
Hairpin  mode
bpftool net show
bpftool map list --bpffs -j | jq 

bpftool map dump id 92 (16 è¿›åˆ¶ c0 192)
cilium bpf  tunnel list
```





```
dma
sk buffer
```















```
tcpdump
cilium monitor -vv
```



```
cilium endpoint list
```





```
è·¨èŠ‚ç‚¹é€šä¿¡ ä»€ä¹ˆæ—¶å€™åšsnat ä»€ä¹ˆæ—¶å€™ä¸åšsnat ï¼Ÿï¼Ÿï¼Ÿ
cidrxxxx
native-routing ä¸‹çš„å‚æ•°
```





```
Conntrack lookup
```



https://cilium.io/blog/2021/05/11/cni-benchmark



```
bpftool net show

tcï¼šhoook
```



```
tc filter show dev lxcxxxxx ingress

tc filter show dev lxcxxxxx engress
```





```
cilium tunnel list
```





## 10 2022-05-04-pass





### Native-Routing





https://docs.cilium.io/en/stable/concepts/networking/routing/#id2











https://isovalent.com/blog/post/2021-12-08-ebpf-servicemesh





```
native routing  è¦æ±‚ï¼Œnode l2 å¯è¾¾
```







```
pod  å¯¹è™šæ‹Ÿæœº
```



```
ç²¾ç®€è·¯ç”±æ¡ç›®
èšåˆ
è·¯ç”±èšåˆ

l3 è·¯ç”± (no nat )æ¯ä¸€è·³ éƒ½ä¼šå˜mac åœ°å€ ä½†æ˜¯ip ä¸å˜
```



```
æŸ¥è¯¢è·¯ç”±è¡¨è§„åˆ™
æœ€é•¿åŒ¹é…ä¼˜å…ˆ  ä¼˜å…ˆæœ€ç²¾ç¡®çš„è·¯ç”±  
```



https://support.huawei.com/hedex/hdx.do?docid=EDOC1100196230&lang=zh&id=ZH-CN_CONCEPT_0177102067









https://docs.cilium.io/en/stable/gettingstarted/bird/





```
bird

yum install net-tools bird bird6 -y



```





### ensp











```
## static  route


ar2240



# AR3

sys
sysname AR3

int g0/0/1
ip a 10.0.0.1 24



int g0/0/0
ip a 192.168.2.61 24

dis thi


# AR4
sys
sysname AR4

int g0/0/1
ip a 10.0.1.1 24


int g0/0/0
ip a 192.168.2.62 24

dis thi





dis ip routing-table


AR3
sys
ip route-static 10.0.1.0 24 192.168.2.62



AR4
sys
ip route-static 10.0.0.0 24 192.168.2.61




### OSPF (åŠ¨æ€è·¯ç”±)

æŸ¥çœ‹ip
dis ip int b


route1
undo ip route-static 10.0.1.0 24 192.168.2.62




sys
ospf router-id 192.168.2.61
area 0
network 0.0.0.0 255.255.255.255
dis thi


dis ospf peer brief
Â·


route2

undo ip route-static 10.0.0.0 24 192.168.2.61


sys
ospf router-id 192.168.2.62
area 0
network 0.0.0.0 255.255.255.255
dis thi


dis ospf peer brief



### BGP  è¾¹ç•Œç½‘å…³åè®®

1
sys
undo ospf 1


bgp 12
router-id 192.168.2.61
peer 192.168.2.62 as-number 12
network  10.0.0.0 255.255.255.0

dis thi




2
sys
undo ospf 1


bgp 12

router-id  192.168.2.62

[AR4-bgp]router-id 192.168.2.62
[AR4-bgp]peer 192.168.2.61 as-number 12

network  10.0.1.0 255.255.255.0


dis thi


dis bgp routing-table

```





bgp



https://support.huawei.com/hedex/hdx.do?docid=EDOC1100196230&lang=zh&id=ZH-CN_CONCEPT_0177102067











```
test Native-Routing
```



https://docs.cilium.io/en/latest/concepts/networking/routing/#native-routing





```
è·¯ç”±èšåˆ
```



01:51





```
Each individual node is made aware of all pod IPs of all other nodes and routes are inserted into the Linux kernel routing table to represent this. If all nodes share a single L2 network, then this can be taken care of by enabling the option auto-direct-node-routes: true. Otherwise, an additional system component such as a BGP daemon must be run to distribute the routes. See the guide Using kube-router to run BGP on how to achieve this using the kube-router project.
```



```
L2  å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚auto-direct-node-routes true
L3 éœ€è¦
```





```
tunnel: disabled: Enable native routing mode.
ipv4-native-routing-cidr: x.x.x.x/y: Set the CIDR in which native routing can be performed  snat??
```





ä¸ºäº†è¿è¡Œæœ¬æœºè·¯ç”±æ¨¡å¼ï¼Œè¿æ¥è¿è¡ŒCiliumçš„ä¸»æœºçš„ç½‘ç»œå¿…é¡»èƒ½å¤Ÿä½¿ç”¨æä¾›ç»™PODæˆ–å…¶ä»–å·¥ä½œè´Ÿè½½çš„åœ°å€è½¬å‘IPæµé‡ã€‚



èŠ‚ç‚¹ä¸Šçš„Linuxå†…æ ¸å¿…é¡»çŸ¥é“å¦‚ä½•è½¬å‘è¿è¡ŒCiliumçš„æ‰€æœ‰èŠ‚ç‚¹çš„PODåŒ…æˆ–å…¶ä»–å·¥ä½œè´Ÿè½½ã€‚è¿™å¯ä»¥é€šè¿‡ä¸¤ç§æ–¹å¼å®ç°ï¼š



èŠ‚ç‚¹æœ¬èº«ä¸çŸ¥é“å¦‚ä½•è·¯ç”±æ‰€æœ‰pod IPï¼Œä½†ç½‘ç»œä¸Šæœ‰ä¸€ä¸ªè·¯ç”±å™¨çŸ¥é“å¦‚ä½•åˆ°è¾¾æ‰€æœ‰å…¶ä»–podã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒLinuxèŠ‚ç‚¹è¢«é…ç½®ä¸ºåŒ…å«æŒ‡å‘æ­¤ç±»è·¯ç”±å™¨çš„é»˜è®¤è·¯ç”±ã€‚è¯¥æ¨¡å‹ç”¨äºäº‘æä¾›å•†ç½‘ç»œé›†æˆã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…è°·æ­Œäº‘ã€AWS ENIå’ŒAzure IPAMã€‚



æ¯ä¸ªèŠ‚ç‚¹éƒ½çŸ¥é“æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹çš„æ‰€æœ‰pod IPï¼Œå¹¶å°†è·¯ç”±æ’å…¥Linuxå†…æ ¸è·¯ç”±è¡¨ä¸­ä»¥è¡¨ç¤ºè¿™ä¸€ç‚¹ã€‚å¦‚æœæ‰€æœ‰èŠ‚ç‚¹å…±äº«ä¸€ä¸ªL2ç½‘ç»œï¼Œåˆ™å¯ä»¥é€šè¿‡å¯ç”¨é€‰é¡¹auto direct node routes:trueæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚å¦åˆ™ï¼Œå¿…é¡»è¿è¡Œé¢å¤–çš„ç³»ç»Ÿç»„ä»¶ï¼ˆå¦‚BGPå®ˆæŠ¤è¿›ç¨‹ï¼‰æ¥åˆ†å‘è·¯ç”±ã€‚è¯·å‚é˜…ä½¿ç”¨kube routerè¿è¡ŒBGPçš„æŒ‡å—ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨kube routeré¡¹ç›®å®ç°è¿™ä¸€ç‚¹ã€‚







```
rpcapd
```





## 11 2022-05-06-pass





Host-Reachable Services  + DSR

https://docs.cilium.io/en/latest/gettingstarted/host-services/#host-reachable-services





```
Host-Reachable Services

å¼€å¯ Host-Reachable Services



kind 

--set hostServices.enable=true

tcpdump -p å…³æ‰æ··æ‚æ¨¡å¼


tcp æŸ¥çœ‹ä¸‰æ¬¡æ¡æ‰‹åœ°å€ 




cilium-agnet --help 

```









------



```

dsr å—åŒ—
```









dsr



https://docs.cilium.io/en/latest/gettingstarted/kubeproxy-free/#direct-server-return-dsr





```
require native-routing
```



```
reverse  snat
```









```
helm install cilium ./cilium \
    --namespace kube-system \
    --set tunnel=disabled \  
    --set autoDirectNodeRoutes=true \
    --set kubeProxyReplacement=strict \
    --set loadBalancer.mode=dsr \
    --set k8sServiceHost=REPLACE_WITH_API_SERVER_IP \
    --set k8sServicePort=REPLACE_WITH_API_SERVER_PORT
```







```
monitor-aggregation: "medium"




  ipv4-native-routing-cidr: 10.0.0.0/8
  monitor-aggregation: "none"
  enable-bpf-masquerade: "true"
  enable-endpoint-routes: "false"
```





node port test 



åªæœ‰http requst 

æ²¡æœ‰http 200 ok

## 12 2022-05-08-pass





cluster mesh









https://cilium.io/blog/2019/03/12/clustermesh/







```
https://cilium.io/blog/2019/03/12/clustermesh/
```



https://docs.cilium.io/en/v1.11/gettingstarted/clustermesh/clustermesh/#clustermesh















```
kind get clusters
```



```
kubectl --context kind-c1 get pods 
```





```
kind: Cluster
apiVersion: Kind.x-k8s.io/v1alpha4
networking:
        disaleDefaultCNI: true
        kubeProxyMode: "ipvs"
nodes:
        - role: control-plane
        - role: worker
        - role: worker
        
        
kind create         
```





https://metallb.universe.tf/



### kind





docker

heml

cilium cli



```
wget  https://github.com/cilium/cilium-cli/releases/download/v0.11.5/cilium-linux-amd64.tar.gz
```





https://kind.sigs.k8s.io/docs/user/quick-start/#installing-with-a-package-manager







```
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.12.0/kind-linux-amd64
chmod +x ./kind
mv ./kind  /usr/local/bin/
```



```
 docker pull kindest/node:v1.23.4
```





```
apt-get install   kubectl=1.23.4-00 -y
```



```
cat >  kind-config1.yaml   <<  EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
networking:
  disableDefaultCNI: true
  podSubnet: "10.10.0.0/16"
  serviceSubnet: "10.11.0.0/16"
EOF
```





```
kind  delete  clusters  c1
```







-----





```
kind create cluster --config ./kind-config1.yaml --name c1  --image kindest/node:v1.23.4
```



```
kubectl cluster-info --context kind-c1
```











```
kubectl config view
```



------



```
kubectl config use-context kind-c1
```



```
helm repo add cilium https://helm.cilium.io/
```



```
helm install cilium cilium/cilium --version 1.11.1 \
  --namespace kube-system \
  --set ipam.mode=kubernetes \
  --set cluster.id=1 \
  --set cluster.name=cluster1 \
  --set hubble.relay.enabled=true \
  --set  hubble.ui.enabled=true  
```





```
helm repo add bitnami https://charts.bitnami.com/bitnami
```





```
cat >  configmap1.yaml   <<  EOF
configInline:
  peers:
  address-pools:
  - name: default
    protocol: layer2
    addresses:
    - 172.18.0.50-172.18.0.99
EOF
```





```
helm install metallb bitnami/metallb \
  --namespace kube-system \
  -f ./configmap1.yaml
```









```
cilium clustermesh enable --create-ca --service-type LoadBalancer

cilium  status
```













```
kubectl get secret --context kind-c1 -n kube-system cilium-ca -o yaml > cilium-ca.yaml
```





-----







```
kind  delete  clusters  c2
```



```
cat >  kind-config2.yaml   <<  EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
networking:
  disableDefaultCNI: true
  podSubnet: "10.20.0.0/16"
  serviceSubnet: "10.21.0.0/16"
EOF
```





```
kind create cluster --config ./kind-config2.yaml --name c2  --image kindest/node:v1.23.4
```





```
kubectl config use-context kind-c2
```



```
helm install cilium cilium/cilium --version=1.11.1 \
  --namespace kube-system \
  --set ipam.mode=kubernetes \
  --set cluster.id=2 \
  --set cluster.name=cluster2 \
  --set hubble.relay.enabled=true \
  --set  hubble.ui.enabled=true  
```









```
cat >  configmap2.yaml   <<  EOF
configInline:
  peers:
  address-pools:
  - name: default
    protocol: layer2
    addresses:
    - 172.18.0.100-172.18.0.149
EOF
```



```
helm install metallb bitnami/metallb \
  --namespace kube-system \
  -f ./configmap2.yaml
```









```
kubectl apply -f cilium-ca.yaml --context kind-c2
```



```
cilium clustermesh enable  --service-type LoadBalancer
```













```
root@172-16-100-7:~# cilium clustermesh  status --context kind-c1
âœ… Cluster access information is available:
  - 172.18.0.50:2379
âœ… Service "clustermesh-apiserver" of type "LoadBalancer" found
ğŸ”Œ Cluster Connections:
ğŸ”€ Global services: [ min:0 / avg:0.0 / max:0 ]


cilium clustermesh  status --context kind-c2
âœ… Cluster access information is available:
  - 172.18.0.100:2379
âœ… Service "clustermesh-apiserver" of type "LoadBalancer" found
ğŸ”Œ Cluster Connections:
ğŸ”€ Global services: [ min:0 / avg:0.0 / max:0 ]
root@172-16-100-7:~# 
```







```
cilium clustermesh connect --context kind-c2 --destination-context kind-c1
```





```
cilium clustermesh  status --context kind-c1

âœ… Cluster access information is available:
  - 172.18.0.50:2379
âœ… Service "clustermesh-apiserver" of type "LoadBalancer" found
âœ… All 2 nodes are connected to all clusters [min:1 / avg:1.0 / max:1]
ğŸ”Œ Cluster Connections:
- cluster2: 2/2 configured, 2/2 connected
ğŸ”€ Global services: [ min:5 / avg:5.0 / max:5 ]






cilium clustermesh  status --context kind-c2


âœ… Cluster access information is available:
  - 172.18.0.100:2379
âœ… Service "clustermesh-apiserver" of type "LoadBalancer" found
âœ… All 2 nodes are connected to all clusters [min:1 / avg:1.0 / max:1]
ğŸ”Œ Cluster Connections:
- cluster1: 2/2 configured, 2/2 connected
ğŸ”€ Global services: [ min:5 / avg:5.0 / max:5 ]



```



éªŒè¯



```
## Verify:
kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/1.11.2/examples/kubernetes/clustermesh/global-service-example/cluster1.yaml --context kind-c1
kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/1.11.2/examples/kubernetes/clustermesh/global-service-example/cluster2.yaml --context kind-c2
```









```
root@172-16-100-7:~# kubectl  get pod  --context=kind-c2    -o wide
NAME                          READY   STATUS    RESTARTS   AGE    IP            NODE        NOMINATED NODE   READINESS GATES
rebel-base-6985d8f76f-gm82t   1/1     Running   0          11m    10.20.1.217   c2-worker   <none>           <none>
rebel-base-6985d8f76f-wttww   1/1     Running   0          11m    10.20.1.82    c2-worker   <none>           <none>
x-wing-6d58648f95-qnxgg       1/1     Running   0          2m2s   10.20.1.236   c2-worker   <none>           <none>
x-wing-6d58648f95-r2xcv       1/1     Running   0          114s   10.20.1.13    c2-worker   <none>           <none>

root@172-16-100-7:~# kubectl  get pod  --context=kind-c1    -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP            NODE        NOMINATED NODE   READINESS GATES
rebel-base-6985d8f76f-6ddwd   1/1     Running   0          12m   10.10.1.196   c1-worker   <none>           <none>
rebel-base-6985d8f76f-j5p7h   1/1     Running   0          12m   10.10.1.234   c1-worker   <none>           <none>
x-wing-6d58648f95-9kf22       1/1     Running   0          12m   10.10.1.218   c1-worker   <none>           <none>
x-wing-6d58648f95-ggbwg       1/1     Running   0          12m   10.10.1.152   c1-worker   <none>           <none>
root@172-16-100-7:~# 
```



```
kubectl  get svc --context kind-c1 | grep 'rebel-base'
rebel-base   ClusterIP   10.11.142.37   <none>        80/TCP    55s
```





```
kubectl  get svc --context kind-c2 | grep 'rebel-base'
rebel-base   ClusterIP   10.21.2.210   <none>        80/TCP    3s
```





x-wing  è®¿é—®  global server   rebel-base



è¿™é‡ŒæŠŠ  kind-c2 çš„  deployment/x-wing å½“ä½œå®¢æˆ·ç«¯  ç”¨ä»»ä½•ç½‘ç»œå¯è¾¾çš„ éƒ½å¯ä»¥å½“ä½œå®¢æˆ·ç«¯

```
kubectl --context kind-c2 exec -ti deployment/x-wing -- curl rebel-base  
```





```
for i in $(seq 1 10000); do kubectl --context kind-c2 exec -ti deployment/x-wing -- curl rebel-base  ;done
```





åˆ é™¤  kind-c1 çš„ pod

```
kubectl scale deployment rebel-base --replicas=0 --context kind-c1
```



```
kubectl  get pod  --context kind-c1
NAME                      READY   STATUS    RESTARTS   AGE
x-wing-6d58648f95-9kf22   1/1     Running   0          18m
x-wing-6d58648f95-ggbwg   1/1     Running   0          18m
root@172-16-100-7:~# 
```



ç”¨ kind-c1 å½“ä½œå®¢æˆ·ç«¯ 

```
for i in $(seq 1 10000); do kubectl --context kind-c1 exec -ti deployment/x-wing -- curl rebel-base  ;done
```



----







##### connectivity test



```
cilium hubble port-forward&  --context kind-c1
```









```
root@172-16-100-7:~# cilium connectivity test --context kind-c1
â„¹ï¸  Single-node environment detected, enabling single-node connectivity test
â„¹ï¸  Monitor aggregation detected, will skip some flow validation steps
âŒ› [kind-c1] Waiting for deployments [client client2 echo-same-node] to become ready...
âŒ› [kind-c1] Waiting for deployments [] to become ready...
âŒ› [kind-c1] Waiting for CiliumEndpoint for pod cilium-test/client-7568bc7f86-ccjtz to appear...
âŒ› [kind-c1] Waiting for CiliumEndpoint for pod cilium-test/client2-686d5f784b-mbsdt to appear...
âŒ› [kind-c1] Waiting for CiliumEndpoint for pod cilium-test/echo-same-node-5767b7b99d-4snb5 to appear...
âŒ› [kind-c1] Waiting for Service cilium-test/echo-same-node to become ready...
âŒ› [kind-c1] Waiting for NodePort 172.18.0.2:30173 (cilium-test/echo-same-node) to become ready...
âŒ› [kind-c1] Waiting for NodePort 172.18.0.3:30173 (cilium-test/echo-same-node) to become ready...
â„¹ï¸  Skipping IPCache check
âŒ› [kind-c1] Waiting for pod cilium-test/client-7568bc7f86-ccjtz to reach default/kubernetes service...
âŒ› [kind-c1] Waiting for pod cilium-test/client2-686d5f784b-mbsdt to reach default/kubernetes service...
ğŸ”­ Enabling Hubble telescope...
âš ï¸  Unable to contact Hubble Relay, disabling Hubble telescope and flow validation: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:4245: connect: connection refused"
â„¹ï¸  Expose Relay locally with:
   cilium hubble enable
   cilium hubble port-forward&
â„¹ï¸  Cilium version: 1.11.1
ğŸƒ Running tests...

[=] Test [no-policies]
......................
[=] Test [allow-all-except-world]
........
[=] Test [client-ingress]
..
[=] Test [echo-ingress]
..
[=] Test [client-egress]
..
[=] Test [to-entities-world]
......
[=] Test [to-cidr-1111]
....
[=] Test [echo-ingress-l7]
..
[=] Test [client-egress-l7]
........
[=] Test [dns-only]
........
[=] Test [to-fqdns]
......
âœ… All 11 tests (70 actions) successful, 0 tests skipped, 0 scenarios skipped.
root@172-16-100-7:~# 
```





```
root@172-16-100-7:~# cilium connectivity test --context kind-c2
â„¹ï¸  Single-node environment detected, enabling single-node connectivity test
â„¹ï¸  Monitor aggregation detected, will skip some flow validation steps
âŒ› [kind-c2] Waiting for deployments [client client2 echo-same-node] to become ready...
âŒ› [kind-c2] Waiting for deployments [] to become ready...
âŒ› [kind-c2] Waiting for CiliumEndpoint for pod cilium-test/client-7568bc7f86-pqq2q to appear...
âŒ› [kind-c2] Waiting for CiliumEndpoint for pod cilium-test/client2-686d5f784b-q4zvp to appear...
âŒ› [kind-c2] Waiting for CiliumEndpoint for pod cilium-test/echo-same-node-5767b7b99d-mpfj5 to appear...
âŒ› [kind-c2] Waiting for Service cilium-test/echo-same-node to become ready...
âŒ› [kind-c2] Waiting for NodePort 172.18.0.5:31763 (cilium-test/echo-same-node) to become ready...
âŒ› [kind-c2] Waiting for NodePort 172.18.0.4:31763 (cilium-test/echo-same-node) to become ready...
â„¹ï¸  Skipping IPCache check
âŒ› [kind-c2] Waiting for pod cilium-test/client-7568bc7f86-pqq2q to reach default/kubernetes service...
âŒ› [kind-c2] Waiting for pod cilium-test/client2-686d5f784b-q4zvp to reach default/kubernetes service...
ğŸ”­ Enabling Hubble telescope...
âš ï¸  Unable to contact Hubble Relay, disabling Hubble telescope and flow validation: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:4245: connect: connection refused"
â„¹ï¸  Expose Relay locally with:
   cilium hubble enable
   cilium hubble port-forward&
â„¹ï¸  Cilium version: 1.11.1
ğŸƒ Running tests...

[=] Test [no-policies]
......................
[=] Test [allow-all-except-world]
........
[=] Test [client-ingress]
..
[=] Test [echo-ingress]
..
[=] Test [client-egress]
..
[=] Test [to-entities-world]
......
[=] Test [to-cidr-1111]
....
[=] Test [echo-ingress-l7]
..
[=] Test [client-egress-l7]
........
[=] Test [dns-only]
........
[=] Test [to-fqdns]
......
âœ… All 11 tests (70 actions) successful, 0 tests skipped, 0 scenarios skipped.
root@172-16-100-7:~# 
```







```
cilium hubble port-forward&  --context kind-c2
```







```
cilium hubble enable --context kind-c1
```



```
cilium connectivity test --context kind-c1
cilium connectivity test --context kind-c2
```



```
kubectl  get pod -n cilium-test  --context kind-c1
kubectl  get pod -n cilium-test  --context kind-c2
```



```
kubectl  delete  ns cilium-test  --context kind-c1
kubectl  delete  ns cilium-test  --context kind-c2
```







```
docker cp /usr/local/bin/cilium  c1-control-plane:/opt/

/opt/cilium connectivity test --context kind-c1


```







-----





å‘½ä»¤è¡Œ





```
kubectl  get pod -A --context kind-c1
kubectl  get pod -A --context kind-c2
```



```
docker exec -it c1-worker   crictl  images
docker exec -it c2-worker   crictl  images
```



```
docker exec -it c2-worker  crictl  pull quay.io/cilium/cilium:1.11.1
```











-----



















----



##### ä¿¡æ¯æ”¶é›†



```
kubectl  get pod -A --context kind-c1
NAMESPACE            NAME                                       READY   STATUS    RESTARTS   AGE
default              rebel-base-6985d8f76f-fd5hf                1/1     Running   0          15m
default              rebel-base-6985d8f76f-xrngp                1/1     Running   0          15m
default              x-wing-6d58648f95-578fj                    1/1     Running   0          15m
default              x-wing-6d58648f95-v59rt                    1/1     Running   0          15m
kube-system          cilium-45gtt                               1/1     Running   0          18m
kube-system          cilium-mkrgl                               1/1     Running   0          18m
kube-system          cilium-operator-67f9b484d8-92pkm           1/1     Running   0          86m
kube-system          cilium-operator-67f9b484d8-9f5wl           1/1     Running   0          56m
kube-system          clustermesh-apiserver-cbd4456bf-5s4nt      2/2     Running   0          23m
kube-system          coredns-64897985d-dqw6w                    1/1     Running   0          88m
kube-system          coredns-64897985d-xbcwh                    1/1     Running   0          88m
kube-system          etcd-c1-control-plane                      1/1     Running   0          88m
kube-system          hubble-relay-85664d47c4-zsf9p              1/1     Running   0          86m
kube-system          hubble-ui-7c6789876c-nkqpg                 3/3     Running   0          56m
kube-system          kube-apiserver-c1-control-plane            1/1     Running   0          88m
kube-system          kube-controller-manager-c1-control-plane   1/1     Running   0          88m
kube-system          kube-proxy-l6sph                           1/1     Running   0          87m
kube-system          kube-proxy-tkrng                           1/1     Running   0          88m
kube-system          kube-scheduler-c1-control-plane            1/1     Running   0          88m
kube-system          metallb-controller-66865674bf-6kn2x        1/1     Running   0          25m
kube-system          metallb-speaker-fc2vb                      1/1     Running   0          25m
local-path-storage   local-path-provisioner-5ddd94ff66-xb6rj    1/1     Running   0          88m







docker exec -it c1-worker   crictl  images

IMAGE                                      TAG                    IMAGE ID            SIZE
docker.io/bitnami/metallb-controller       0.12.1-debian-10-r61   81d1a3e4dc8ad       43.9MB
docker.io/bitnami/metallb-speaker          0.12.1-debian-10-r62   da61e66ac933a       44.8MB
docker.io/cilium/json-mock                 1.2                    094a89b064ebc       62.5MB
docker.io/kindest/kindnetd                 v20211122-a2c10462     ba113d2047d43       40.9MB
docker.io/rancher/local-path-provisioner   v0.0.14                e422121c9c5f9       13.4MB
k8s.gcr.io/build-image/debian-base         buster-v1.7.2          19bad6b08adae       21.1MB
k8s.gcr.io/coredns/coredns                 v1.8.6                 a4ca41631cc7a       13.6MB
k8s.gcr.io/etcd                            3.5.1-0                25f8c7f3da61c       98.9MB
k8s.gcr.io/kube-apiserver                  v1.23.4                0e16468a4aa36       79.6MB
k8s.gcr.io/kube-controller-manager         v1.23.4                65dd3c630e90e       68.1MB
k8s.gcr.io/kube-proxy                      v1.23.4                163898317bef2       113MB
k8s.gcr.io/kube-scheduler                  v1.23.4                696ef30502b11       54.8MB
k8s.gcr.io/pause                           3.6                    6270bb605e12e       302kB
docker.io/envoyproxy/envoy                 <none>                 fe28da4ca57ab       51.4MB
docker.io/library/nginx                    1.15.8                 f09fe80eb0e75       44.7MB
quay.io/cilium/cilium                      <none>                 11553cb737845       157MB
quay.io/cilium/clustermesh-apiserver       <none>                 8f52e3ea3edf9       15.5MB
quay.io/cilium/hubble-relay                <none>                 d42689b386ff3       10.2MB
quay.io/cilium/hubble-ui-backend           <none>                 a029b5706c2d2       14.4MB
quay.io/cilium/hubble-ui                   <none>                 154fa38916304       12.2MB
quay.io/cilium/operator-generic            <none>                 21681b3deb25a       16.3MB
quay.io/coreos/etcd                        v3.4.13                d1985d4043858       32.6MB




docker exec -it c1-control-plane    crictl  images


IMAGE                                      TAG                  IMAGE ID            SIZE
docker.io/kindest/kindnetd                 v20211122-a2c10462   ba113d2047d43       40.9MB
k8s.gcr.io/coredns/coredns                 v1.8.6               a4ca41631cc7a       13.6MB
k8s.gcr.io/etcd                            3.5.1-0              25f8c7f3da61c       98.9MB
k8s.gcr.io/kube-apiserver                  v1.23.4              0e16468a4aa36       79.6MB
quay.io/cilium/cilium                      <none>               11553cb737845       157MB
k8s.gcr.io/kube-proxy                      v1.23.4              163898317bef2       113MB
quay.io/cilium/operator-generic            <none>               21681b3deb25a       16.3MB
docker.io/rancher/local-path-provisioner   v0.0.14              e422121c9c5f9       13.4MB
k8s.gcr.io/build-image/debian-base         buster-v1.7.2        19bad6b08adae       21.1MB
k8s.gcr.io/kube-controller-manager         v1.23.4              65dd3c630e90e       68.1MB
k8s.gcr.io/kube-scheduler                  v1.23.4              696ef30502b11       54.8MB
k8s.gcr.io/pause                           3.6                  6270bb605e12e       302kB









kubectl  get pod -A --context kind-c2

NAMESPACE            NAME                                       READY   STATUS    RESTARTS   AGE
default              rebel-base-6985d8f76f-25sxq                1/1     Running   0          16m
default              rebel-base-6985d8f76f-zpzd2                1/1     Running   0          16m
default              x-wing-6d58648f95-t6dfl                    1/1     Running   0          16m
default              x-wing-6d58648f95-tw54m                    1/1     Running   0          16m
kube-system          cilium-4b4rw                               1/1     Running   0          19m
kube-system          cilium-operator-67f9b484d8-49lpr           1/1     Running   0          37m
kube-system          cilium-operator-67f9b484d8-h8ncw           1/1     Running   0          37m
kube-system          cilium-wsxzg                               1/1     Running   0          19m
kube-system          clustermesh-apiserver-866d964d6c-c4rdf     2/2     Running   0          21m
kube-system          coredns-64897985d-7wqgr                    1/1     Running   0          38m
kube-system          coredns-64897985d-ckgph                    1/1     Running   0          38m
kube-system          etcd-c2-control-plane                      1/1     Running   0          38m
kube-system          hubble-relay-85664d47c4-k8jhf              1/1     Running   0          37m
kube-system          hubble-ui-7c6789876c-h8rw5                 3/3     Running   0          37m
kube-system          kube-apiserver-c2-control-plane            1/1     Running   0          38m
kube-system          kube-controller-manager-c2-control-plane   1/1     Running   0          38m
kube-system          kube-proxy-6d89k                           1/1     Running   0          38m
kube-system          kube-proxy-dx28z                           1/1     Running   0          38m
kube-system          kube-scheduler-c2-control-plane            1/1     Running   0          38m
kube-system          metallb-controller-66865674bf-xdh9c        1/1     Running   0          26m
kube-system          metallb-speaker-zd5v4                      1/1     Running   0          26m
local-path-storage   local-path-provisioner-5ddd94ff66-vpbjb    1/1     Running   0          38m





docker exec -it c2-worker   crictl  images

IMAGE                                      TAG                    IMAGE ID            SIZE
docker.io/bitnami/metallb-controller       0.12.1-debian-10-r61   81d1a3e4dc8ad       43.9MB
docker.io/bitnami/metallb-speaker          0.12.1-debian-10-r62   da61e66ac933a       44.8MB
docker.io/cilium/json-mock                 1.2                    094a89b064ebc       62.5MB
quay.io/cilium/hubble-ui                   <none>                 154fa38916304       12.2MB
docker.io/kindest/kindnetd                 v20211122-a2c10462     ba113d2047d43       40.9MB
docker.io/library/nginx                    1.15.8                 f09fe80eb0e75       44.7MB
quay.io/cilium/cilium                      <none>                 11553cb737845       157MB
docker.io/rancher/local-path-provisioner   v0.0.14                e422121c9c5f9       13.4MB
k8s.gcr.io/build-image/debian-base         buster-v1.7.2          19bad6b08adae       21.1MB
k8s.gcr.io/coredns/coredns                 v1.8.6                 a4ca41631cc7a       13.6MB
k8s.gcr.io/etcd                            3.5.1-0                25f8c7f3da61c       98.9MB
k8s.gcr.io/kube-apiserver                  v1.23.4                0e16468a4aa36       79.6MB
k8s.gcr.io/kube-controller-manager         v1.23.4                65dd3c630e90e       68.1MB
k8s.gcr.io/kube-proxy                      v1.23.4                163898317bef2       113MB
k8s.gcr.io/kube-scheduler                  v1.23.4                696ef30502b11       54.8MB
docker.io/envoyproxy/envoy                 <none>                 fe28da4ca57ab       51.4MB
quay.io/cilium/hubble-relay                <none>                 d42689b386ff3       10.2MB
quay.io/cilium/hubble-ui-backend           <none>                 a029b5706c2d2       14.4MB
quay.io/cilium/operator-generic            <none>                 21681b3deb25a       16.3MB
k8s.gcr.io/pause                           3.6                    6270bb605e12e       302kB
quay.io/cilium/clustermesh-apiserver       <none>                 8f52e3ea3edf9       15.5MB
quay.io/coreos/etcd                        v3.4.13                d1985d4043858       32.6MB







docker exec -it c2-control-plane   crictl  images

IMAGE                                      TAG                  IMAGE ID            SIZE
quay.io/cilium/cilium                      <none>               11553cb737845       157MB
docker.io/kindest/kindnetd                 v20211122-a2c10462   ba113d2047d43       40.9MB
k8s.gcr.io/build-image/debian-base         buster-v1.7.2        19bad6b08adae       21.1MB
quay.io/cilium/operator-generic            <none>               21681b3deb25a       16.3MB
docker.io/rancher/local-path-provisioner   v0.0.14              e422121c9c5f9       13.4MB
k8s.gcr.io/coredns/coredns                 v1.8.6               a4ca41631cc7a       13.6MB
k8s.gcr.io/etcd                            3.5.1-0              25f8c7f3da61c       98.9MB
k8s.gcr.io/kube-apiserver                  v1.23.4              0e16468a4aa36       79.6MB
k8s.gcr.io/kube-controller-manager         v1.23.4              65dd3c630e90e       68.1MB
k8s.gcr.io/kube-proxy                      v1.23.4              163898317bef2       113MB
k8s.gcr.io/kube-scheduler                  v1.23.4              696ef30502b11       54.8MB
k8s.gcr.io/pause                           3.6                  6270bb605e12e       302kB



```









```
root@172-16-100-7:~# cilium clustermesh status  --context kind-c1
âœ… Cluster access information is available:
  - 172.18.0.50:2379
âœ… Service "clustermesh-apiserver" of type "LoadBalancer" found
âœ… All 2 nodes are connected to all clusters [min:1 / avg:1.0 / max:1]
ğŸ”Œ Cluster Connections:
- cluster2: 2/2 configured, 2/2 connected
ğŸ”€ Global services: [ min:6 / avg:6.0 / max:6 ]


root@172-16-100-7:~# cilium clustermesh status  --context kind-c2
âœ… Cluster access information is available:
  - 172.18.0.100:2379
âœ… Service "clustermesh-apiserver" of type "LoadBalancer" found
âœ… All 2 nodes are connected to all clusters [min:1 / avg:1.0 / max:1]
ğŸ”Œ Cluster Connections:
- cluster1: 2/2 configured, 2/2 connected
ğŸ”€ Global services: [ min:6 / avg:6.0 / max:6 ]
root@172-16-100-7:~# 
```







```
root@172-16-100-7:~# cilium status  --context kind-c1
    /Â¯Â¯\
 /Â¯Â¯\__/Â¯Â¯\    Cilium:         OK
 \__/Â¯Â¯\__/    Operator:       OK
 /Â¯Â¯\__/Â¯Â¯\    Hubble:         OK
 \__/Â¯Â¯\__/    ClusterMesh:    OK
    \__/

DaemonSet         cilium                   Desired: 2, Ready: 2/2, Available: 2/2
Deployment        cilium-operator          Desired: 2, Ready: 2/2, Available: 2/2
Deployment        hubble-relay             Desired: 1, Ready: 1/1, Available: 1/1
Deployment        hubble-ui                Desired: 1, Ready: 1/1, Available: 1/1
Deployment        clustermesh-apiserver    Desired: 1, Ready: 1/1, Available: 1/1
Containers:       hubble-relay             Running: 1
                  hubble-ui                Running: 1
                  clustermesh-apiserver    Running: 1
                  cilium                   Running: 2
                  cilium-operator          Running: 2
Cluster Pods:     11/11 managed by Cilium
Image versions    cilium                   quay.io/cilium/cilium:v1.11.1@sha256:251ff274acf22fd2067b29a31e9fda94253d2961c061577203621583d7e85bd2: 2
                  cilium-operator          quay.io/cilium/operator-generic:v1.11.1@sha256:977240a4783c7be821e215ead515da3093a10f4a7baea9f803511a2c2b44a235: 2
                  hubble-relay             quay.io/cilium/hubble-relay:v1.11.1@sha256:23d40b2a87a5bf94e0365bd9606721c96f78b8304b61725dca45a0b8a6048203: 1
                  hubble-ui                quay.io/cilium/hubble-ui:v0.8.5@sha256:4eaca1ec1741043cfba6066a165b3bf251590cf4ac66371c4f63fbed2224ebb4: 1
                  hubble-ui                quay.io/cilium/hubble-ui-backend:v0.8.5@sha256:2bce50cf6c32719d072706f7ceccad654bfa907b2745a496da99610776fe31ed: 1
                  hubble-ui                docker.io/envoyproxy/envoy:v1.18.4@sha256:e5c2bb2870d0e59ce917a5100311813b4ede96ce4eb0c6bfa879e3fbe3e83935: 1
                  clustermesh-apiserver    quay.io/coreos/etcd:v3.4.13: 1
                  clustermesh-apiserver    quay.io/cilium/clustermesh-apiserver:v1.11.1@sha256:5732f2ce99096d1c740f3805260dbcfefbe6d7d18d1ac07707ff4ef9536b0ec6: 1
root@172-16-100-7:~# 
root@172-16-100-7:~# 
root@172-16-100-7:~# 


root@172-16-100-7:~# cilium status  --context kind-c2
    /Â¯Â¯\
 /Â¯Â¯\__/Â¯Â¯\    Cilium:         OK
 \__/Â¯Â¯\__/    Operator:       OK
 /Â¯Â¯\__/Â¯Â¯\    Hubble:         OK
 \__/Â¯Â¯\__/    ClusterMesh:    OK
    \__/

DaemonSet         cilium                   Desired: 2, Ready: 2/2, Available: 2/2
Deployment        cilium-operator          Desired: 2, Ready: 2/2, Available: 2/2
Deployment        hubble-relay             Desired: 1, Ready: 1/1, Available: 1/1
Deployment        hubble-ui                Desired: 1, Ready: 1/1, Available: 1/1
Deployment        clustermesh-apiserver    Desired: 1, Ready: 1/1, Available: 1/1
Containers:       cilium                   Running: 2
                  cilium-operator          Running: 2
                  hubble-relay             Running: 1
                  hubble-ui                Running: 1
                  clustermesh-apiserver    Running: 1
Cluster Pods:     11/11 managed by Cilium
Image versions    cilium                   quay.io/cilium/cilium:v1.11.1@sha256:251ff274acf22fd2067b29a31e9fda94253d2961c061577203621583d7e85bd2: 2
                  cilium-operator          quay.io/cilium/operator-generic:v1.11.1@sha256:977240a4783c7be821e215ead515da3093a10f4a7baea9f803511a2c2b44a235: 2
                  hubble-relay             quay.io/cilium/hubble-relay:v1.11.1@sha256:23d40b2a87a5bf94e0365bd9606721c96f78b8304b61725dca45a0b8a6048203: 1
                  hubble-ui                quay.io/cilium/hubble-ui:v0.8.5@sha256:4eaca1ec1741043cfba6066a165b3bf251590cf4ac66371c4f63fbed2224ebb4: 1
                  hubble-ui                quay.io/cilium/hubble-ui-backend:v0.8.5@sha256:2bce50cf6c32719d072706f7ceccad654bfa907b2745a496da99610776fe31ed: 1
                  hubble-ui                docker.io/envoyproxy/envoy:v1.18.4@sha256:e5c2bb2870d0e59ce917a5100311813b4ede96ce4eb0c6bfa879e3fbe3e83935: 1
                  clustermesh-apiserver    quay.io/coreos/etcd:v3.4.13: 1
                  clustermesh-apiserver    quay.io/cilium/clustermesh-apiserver:v1.11.1@sha256:5732f2ce99096d1c740f3805260dbcfefbe6d7d18d1ac07707ff4ef9536b0ec6: 1
root@172-16-100-7:~# 
```



## 13 2022-05-09-pass





```
cilium bpf tunnel list

cilium service list

cilium identity list
```





https://01.org/blogs/xuyizhou/2021/accelerate-istio-dataplane-ebpf-part-1







https://thenewstack.io/how-ebpf-streamlines-the-service-mesh/





https://isovalent.com/blog/post/2021-12-08-ebpf-servicemesh







https://isovalent.com/blog/post/2022-05-03-servicemesh-security



----------------------------------- cilium end------------





## 14  2022-05-11-calico-pass



calico



https://projectcalico.docs.tigera.io/getting-started/kubernetes/quickstart





```
calicoctl  node status  --allow-version-mismatch
```





```
ospf bgp 
```







https://projectcalico.docs.tigera.io/maintenance/troubleshoot/vpp





vpp+dpdk





```
ecmp 
```





https://projectcalico.docs.tigera.io/reference/architecture/design/l3-interconnect-fabric





ospf/bgp  + ensp 



calico  åŸºç¡€æ¶æ„





https://projectcalico.docs.tigera.io/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico



https://projectcalico.docs.tigera.io/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico-with-kubernetes-api-datastore-50-nodes-or-less



```
curl https://projectcalico.docs.tigera.io/manifests/calico.yaml -O
kubectl apply -f calico.yaml
```









```
cat >  kind-ex.yaml   <<  EOF
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
        - role: control-plane
        - role: worker
        - role: worker
networking:
        disableDefaultCNI: true
        kubeProxyMode: "ipvs"
EOF
```









```
kind create cluster --config ./kind-ex.yaml  --name calico  --image kindest/node:v1.23.4
```



```
kubectl get pod -A  --context kind-calico
```



```
kubectl apply -f calico.yaml  --context kind-calico
```





```
kubectl  api-resources   --context kind-calico  |  grep ipam
ipamblocks                                     crd.projectcalico.org/v1               false        IPAMBlock
ipamconfigs                                    crd.projectcalico.org/v1               false        IPAMConfig
ipamhandles                                    crd.projectcalico.org/v1               false        IPAMHandle
root@172-16-100-7:~# 


kubectl  api-resources   --context kind-calico  |  grep ippools
ippools                                        crd.projectcalico.org/v1               false        IPPool
```







```
ps -ef | grep bird
root      576388  576293  0 20:50 ?        00:00:00 runsv bird
root      576390  576293  0 20:50 ?        00:00:00 runsv bird6
root      576829  576390  0 20:50 ?        00:00:01 bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg
```







## 15 2022-05-13-calico-pass



calico-ipip





å¼€å¯IPIP ç¦ç”¨ vxlan



https://projectcalico.docs.tigera.io/reference/node/configuration







```
            # Enable IPIP
            - name: CALICO_IPV4POOL_IPIP
              value: "Always"
            # Enable or Disable VXLAN on the default IP pool.
            - name: CALICO_IPV4POOL_VXLAN
              value: "Never"
            # Enable or Disable VXLAN on the default IPv6 IP pool.
            - name: CALICO_IPV6POOL_VXLAN
              value: "Never"
```



calicoctl install



```
curl -L https://github.com/projectcalico/calico/releases/download/v3.23.1/calicoctl-linux-amd64 -o calicoctl
```





```
  Valid resource types are:

    * bgpConfiguration
    * bgpPeer
    * felixConfiguration
    * globalNetworkPolicy
    * globalNetworkSet
    * hostEndpoint
    * ipPool
    * ipReservation
    * kubeControllersConfiguration
    * networkPolicy
    * networkSet
    * node
    * profile
    * workloadEndpoint
```



```
calicoctl   get workloadEndpoint -o wide -A
```







```
ee:ee:ee:ee:ee:ee
```



```
calicoctl   get wep -o wide -A
```









```
cat > test-nginx.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx01
spec:
  containers:
  - name: nginx
    image: yimtune/nginx:1.21.6
    imagePullPolicy: IfNotPresent
    ports:
    - containerPort: 80
EOF
```





```
kubectl apply  -f test-nginx.yaml
```



```
kubectl   exec -it nginx01  --  route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         169.254.1.1     0.0.0.0         UG    0      0        0 eth0
169.254.1.1     0.0.0.0         255.255.255.255 UH    0      0        0 eth0
```





https://projectcalico.docs.tigera.io/reference/faq#why-does-my-container-have-a-route-to-16925411







```
æºIP 10.244.9.129/32 æºmac ce:33:a7:1b:ba:db

ç›®çš„ip 114.114.114.114  ç›®çš„mac ?? ee:ee:ee:ee:ee:ee
```



```
kubectl   exec -it nginx01  --  arp -n
Address                  HWtype  HWaddress           Flags Mask            Iface
169.254.1.1              ether   ee:ee:ee:ee:ee:ee   C                     eth0
172.18.0.2               ether   ee:ee:ee:ee:ee:ee   C                     eth0
root@172-16-100-7:~# 


kubectl   exec -it nginx01  --  arp -a
? (169.254.1.1) at ee:ee:ee:ee:ee:ee [ether] on eth0
calico-worker.kind (172.18.0.2) at ee:ee:ee:ee:ee:ee [ether] on eth0





```





### proxy arp





![image-20220518152514277](kubernetes-network.assets/image-20220518152514277.png)





pc1 pc2 ä½äºåŒä¸€ä¸ªç½‘ç»œ







https://support.huawei.com/enterprise/zh/doc/EDOC1000178148/84487237/configuring-proxy-arp





```
AR3260

sys


int g0/0/0
ip a 192.168.1.254 24


int g0/0/1
ip a 192.168.2.254 24




[Huawei-GigabitEthernet0/0/1]dis ip int b
*down: administratively down
^down: standby
(l): loopback
(s): spoofing
The number of interface that is UP in Physical is 3
The number of interface that is DOWN in Physical is 1
The number of interface that is UP in Protocol is 3
The number of interface that is DOWN in Protocol is 1

Interface                         IP Address/Mask      Physical   Protocol  
GigabitEthernet0/0/0              192.168.1.254/24     up         up        
GigabitEthernet0/0/1              192.168.2.254/24     up         up        
GigabitEthernet0/0/2              unassigned           down       down      
NULL0                             unassigned           up         up(s)     
[Huawei-GigabitEthernet0/0/1]





int g0/0/0
arp-proxy enable

int g0/0/1
arp-proxy enable





```



linux



```
echo 1 > /proc/sys/net/ipv4/conf/eth0/proxy_arp
```





### mac ee:ee:ee:ee:ee:ee



åŒèŠ‚ç‚¹ pod é€šä¿¡



```
10.244.9.129/32   
```



ä¸¤ä¸ªä¸ªpod 32ä½æ©ç  ä¸æ˜¯åŒä¸€ç½‘æ®µ   èµ°è·¯ç”± 





åŒèŠ‚ç‚¹pod è®¾å¤‡æ ‡è®°è¡¨



|                 |                           |      |      |      |
| --------------- | ------------------------- | ---- | ---- | ---- |
| pod1-eht0       | pod1 eht0 ç½‘å¡            |      |      |      |
| pod1-eht0-ip    | pod1 eht0 ç½‘å¡ip          |      |      |      |
| pod1-eht0-mac   | pod1 eht0 ç½‘å¡mac         |      |      |      |
| pod1-gw-ip      | pod1 é»˜è®¤ç½‘å…³ip           |      |      |      |
| pod1-gw-mac     | pod1 é»˜è®¤ç½‘å…³mac          |      |      |      |
| pod1-calico-mac | pod1 eht0 å¯¹ç«¯è®¾å¤‡ç½‘å¡mac |      |      |      |
| pod1-calico-ip  | pod1 eht0 å¯¹ç«¯è®¾å¤‡ç½‘å¡ip  |      |      |      |
| pod1-calico     | pod1 eht0 å¯¹ç«¯è®¾å¤‡ç½‘å¡    |      |      |      |
|                 |                           |      |      |      |
|                 |                           |      |      |      |
| pod2-eht0-ip    | pod2 eht0 ç½‘å¡ip          |      |      |      |
| pod2-eht0-mac   | pod2 eht0 ç½‘å¡mac         |      |      |      |
| pod2-gw-ip      | pod2 é»˜è®¤ç½‘å…³ip           |      |      |      |
| pod2-gw-mac     | pod2 é»˜è®¤ç½‘å…³mac          |      |      |      |
| pod2-calico-mac | pod2 eht0 å¯¹ç«¯è®¾å¤‡ç½‘å¡ip  |      |      |      |
| pod2-calico-ip  | pod2 eht0 å¯¹ç«¯è®¾å¤‡ç½‘å¡mac |      |      |      |
|                 |                           |      |      |      |
|                 |                           |      |      |      |
|                 |                           |      |      |      |





### pod1-eht0-ip        ping       pod2-eht0-ip è¿‡ç¨‹



```
1 pod1-eht0-ip     pod2-eht0-ip   éƒ½æ˜¯32ä½æ©ç  éœ€è¦èµ°ç½‘å…³  pod1-gw-ip  å››å…ƒç»„è§ä¸‹

æºip     pod1-eht0-ip   æºmac    pod1-eht0-mac
ç›®çš„ip    pod2-eht0-ip  ç›®çš„mac   pod1-gw-macï¼ˆä¸åœ¨ä¸€ä¸ªç½‘æ®µï¼Œç›®çš„macæ˜¯ä¸‹ä¸€è·³macï¼‰


2  pod1æœ¬åœ°æ²¡æœ‰ç¼“å­˜ pod1-gw-mac  ä»æ¥å£ pod1-eht0 å‘é€arp å¹¿æ’­   è¯·æ±‚ pod1-gw-ip å¯¹åº”çš„mac


3  pod1-calico ä½œä¸º pod1-eht0 çš„å¯¹ç«¯è®¾å¤‡ï¼Œè¢«åŠ¨çš„æ”¶åˆ°äº† pod1-eht0è®¾å¤‡å‘é€çš„ arp è¯·æ±‚å¹¿æ’­


4  pod1-calico è®¾å¤‡å¼€å¯äº† arp-proxy      
cat  /proc/sys/net/ipv4/conf/cali3362fc7d0e7/proxy_arp
1

pod1-calico æ¥å£ä¸Šå¹¶æ²¡æœ‰  pod1-gw-ip ä½†æ˜¯æ­¤æ—¶pod1-calicoå…·å¤‡è¿”å›è‡ªå·±macåœ°å€çš„èƒ½åŠ›äº†

ç–‘é—®ï¼Ÿï¼Ÿï¼Ÿ

è™½ç„¶å¼€å¯äº† proxy_arp ä½†ä¹Ÿéœ€è¦åœ¨è‡ªå·±å¯è¾¾ç›®çš„ipçš„æƒ…å†µä¸‹æ‰èƒ½è¿”å›
è¿™é‡Œæ²¡æœ‰ä»»ä½•è®¾å¤‡æœ‰è¿™ä¸ª  pod1-gw-ip  ä¸ºä»€ä¹ˆè¿˜å…è®¸  pod1-calicoè¿”å›è‡ªå·±çš„macå‘¢



```





















## 16 2022-03-15-calico-pass



https://projectcalico.docs.tigera.io/networking/vxlan-ipip#configure-ip-in-ip-encapsulation-for-all-inter-workload-traffic



```
å½“ipipModeè®¾ç½®ä¸ºAlwaysæ—¶ï¼ŒCalicoå°†æ¥è‡ªå¯ç”¨Calicoçš„ä¸»æœºçš„æ‰€æœ‰æµé‡ä½¿ç”¨IP-in-IPè·¯ç”±åˆ°IPæ± ä¸­çš„æ‰€æœ‰Calicoç½‘ç»œå®¹å™¨å’ŒVMã€‚
```







```
calicoctl  get ippool -o wide
kubectl api-resouce | grep calico

calicoctl get ippool -o yaml
ipipMode: Always
vxlanMode: Never
natOutgoing: true å‡ºå…¬ç½‘ ä½¿ç”¨nat

nodeSelector: all()

blockSize: 26  26ä½æ©ç 
ä¸€ä¸ª24ä½æ©ç çš„ç½‘ç»œå¯ä»¥åˆ†æˆ4ä¸ª26ä½æ©ç çš„ç½‘ç»œ

00
01
10
11

è·¯ç”±èšåˆ 


```





ipip éœ€è¦ ç”¨bgpæŠ€æœ¯



```
Calicoâ€™s IP in IP implementation uses BGP between Calico nodes.
```





```
calicoctl node status
```





ns  æ¼”ç¤ºç›´è¿è·¯ç”±

äº¤æ¢è·¯ç”± ï¼š ç½‘å…³éƒ½æ˜¯0        0.0.0.0



```
ip tunnel add 
```



```
tcpdump
link-type RAW    (RAW IP)
link-type EN10MB (Ethernet)

Raw packet data
```





linux ipip

```
tun/tap ï¼Ÿï¼Ÿï¼Ÿ


tun åªæœ‰ip æ²¡æœ‰mac


tun /ç”¨æˆ·ç©ºé—´ å†…æ ¸ç©ºé—´ï¼Ÿï¼Ÿ 

```



calico ipip

not save node  pod 



```
ip -d link show tanl0
ipip 
```



```
route -n


 destination       gateway           genmask         interface
10.244.200.192    172.16.1.12      255.255.255.192     tunl0
```



```
tcpdump è§‚å¯Ÿ mac å‰¥ç¦»ç‚¹
```





## 17 2022-05-16-calico-pass



vxlan



### ensp vxlan





https://support.huawei.com/enterprise/zh/doc/EDOC1000178185/4fef8bd9





![image-20220520105237368](kubernetes-network.assets/image-20220520105237368.png)













| è®¾å¤‡å  |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |
| ------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| CE12800 |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |
| S5700   |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |
|         |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |
|         |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |
|         |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |





#### é…ç½®  lsw1     lsw2

```
LSW1

sys
sysname LSW1

åˆ›å»ºvlan  10

vlan 10  // åˆ›å»ºvlan10 
q        // é€€å‡º

int g0/0/2
port link-type access
port default vlan  10



int g0/0/1
port link-type trunk
port trunk  allow-pass vlan all






LSW2
sys
sysname LSW2


```







##### é…ç½®  CE1 CE2  äº’è”IP





```
CE1

system-view immediately
sysname CE1


int GE 1/0/1


å¼€å¯ç«¯å£
[CE1-GE1/0/1]undo shutdown 


å°†è¯¥ç«¯å£å˜æˆ3å±‚çš„ æŠŠäºŒå±‚å£é…ç½®æˆä¸‰å±‚å£ å¦åˆ™åé¢æ— æ³•é…ç½®ip
undo portswitch



é…ç½®ip

[CE1-GE1/0/1]ip a 10.1.1.1 24
[CE1-GE1/0/1]dis thi
#
interface GE1/0/1
 undo portswitch
 undo shutdown
 ip address 10.1.1.1 255.255.255.0
#
return
[CE1-GE1/0/1]

```







```
CE2

system-view immediately
sysname CE2


int GE 1/0/1


å¼€å¯ç«¯å£
[CE1-GE1/0/1]undo shutdown 

å°†è¯¥ç«¯å£å˜æˆ3å±‚çš„ æŠŠäºŒå±‚å£é…ç½®æˆä¸‰å±‚å£ å¦åˆ™åé¢æ— æ³•é…ç½®ip
undo portswitch



é…ç½®ip

ip a 10.1.1.2 24


[CE2-GE1/0/1]dis thi
#
interface GE1/0/1
 undo portswitch
 undo shutdown
 ip address 10.1.1.2 255.255.255.0
#
return
[CE2-GE1/0/1]
```







```
dis ip int b
```









##### CE1 CE2 é…ç½®loopback





```
CE1 é…ç½® LO

system-view immediately 
int LoopBack0   // int l0

ip a 1.1.1.1 32
```





```
CE2 é…ç½® LO


system-view immediately 
int LoopBack0   // int l0

ip a 2.2.2.2  32

```









##### é…ç½® CE1 CE2   ospf  ä½¿å›ç¯å£äº’é€š



```
CE1 é…ç½®ospf
system-view immediately 


ospf 1 router-id  1.1.1.1

å®£å‘Šæ‰€æœ‰ç½‘ç»œ
a 0
network 0.0.0.0 255.255.255.255





CE2 é…ç½®ospf
system-view immediately 


ospf 1 router-id  2.2.2.2

å®£å‘Šæ‰€æœ‰ç½‘ç»œ
a 0
network 0.0.0.0 255.255.255.255




CE1 CE2 ç¡®è®¤

dis ospf peer  brief 

äº’ç›¸ping lo0 æ˜¯é€šçš„
```











#### é…ç½®vlan ç”¨çš„æ¥å£  å…è®¸ vlan 10 é€šè¿‡

##### CE1

```
system-view  immediately 



int g 1/0/0
undo shutdown
q



bridge-domain 10
vxlan vni 10



int GE 1/0/0.10 mode l2


int GE 1/0/0.10
encapsulation dot1q vid 10


bridge-domain  10


[CE1-GE1/0/0.10]dis thi
#
interface GE1/0/0.10 mode l2
 encapsulation dot1q vid 10
 bridge-domain 10
#
return
```





##### CE2



```
system-view  immediately 



int g 1/0/0
undo shutdown
q



bridge-domain 10
vxlan vni 10
q


int GE 1/0/0.10 mode l2

encapsulation dot1q vid 10



[CE1-GE1/0/0.10]bridge-domain  10


[CE1-GE1/0/0.10]dis thi
#
interface GE1/0/0.10 mode l2
 encapsulation dot1q vid 10
 bridge-domain 10
#
return
```





#### é…ç½®vxlan





```
CE1

system-view  immediately 

int Nve 1

source 1.1.1.1
[CE1-Nve1]vni 10 head-end peer-list  2.2.2.2 // vni 10 æ˜¯ä¸Šä¸€æ­¥å®šä¹‰çš„
[CE1-Nve1]dis thi
#
interface Nve1
 source 1.1.1.1
 vni 10 head-end peer-list 2.2.2.2
#
return




[CE1-Nve1]dis vxlan  vni 10 verbose 
    BD ID                  : 10
    State                  : up
    NVE                    : 19
    Source Address         : 1.1.1.1
    Source IPv6 Address    : -
    UDP Port               : 4789
    BUM Mode               : head-end
    Group Address          : -
    Peer List              : 2.2.2.2 
    IPv6 Peer List         : -
[CE1-Nve1]







CE2


system-view  immediately 

int Nve 1

source 2.2.2.2
[CE1-Nve1]vni 10 head-end peer-list  1.1.1.1 // vni 10 æ˜¯ä¸Šä¸€æ­¥å®šä¹‰çš„
[CE1-Nve1]dis thi
#
interface Nve1
 source 2.2.2.2
 vni 10 head-end peer-list 1.1.1.1
#
return






[CE2-Nve1]dis vxlan vni 10 verbose 
    BD ID                  : 10
    State                  : up
    NVE                    : 18
    Source Address         : 2.2.2.2
    Source IPv6 Address    : -
    UDP Port               : 4789
    BUM Mode               : head-end
    Group Address          : -
    Peer List              : 1.1.1.1 
    IPv6 Peer List         : -
[CE2-Nve1]




```





##### test 

pc1 ping pc2

pc2 ping pc1









###  calico-vxlan-always





https://projectcalico.docs.tigera.io/networking/vxlan-ipip#configure-vxlan-encapsulation-for-all-inter-workload-traffic





https://projectcalico.docs.tigera.io/getting-started/kubernetes/managed-public-cloud/eks#install-eks-with-calico-networking







```
kubectl apply -f https://projectcalico.docs.tigera.io/manifests/calico-vxlan.yaml
```









https://projectcalico.docs.tigera.io/getting-started/kubernetes/installation/config-options#switching-from-ip-in-ip-to-vxlan







- Replace `calico_backend: "bird"` with `calico_backend: "vxlan"`.

å»æ‰bird å¥åº·æ£€æŸ¥

```
Comment out the line - -bird-ready and - -bird-live 
```







ä¸åŒèŠ‚ç‚¹ pod



vxlan ä¸ç”¨bgp

```
calicoctl node status
None of the BGP backend processes (BIRD or GoBGP) are running
```





æ¢³ç†è¡¨æ ¼



vxlan è®¾å¤‡

èŠ‚ç‚¹è·¯ç”±è¡¨



```
calicoctl get wep -o wide
tcpdump -pne xxx

vxlan æºç«¯å£0 ä»£è¡¨éšæœºï¼Ÿ
```







```
fdbè¡¨
iptables è¡¨
è·¯ç”±è¡¨
arp è¡¨
```





## 18 2022-05-18-calico-pass





https://projectcalico.docs.tigera.io/networking/bgp



#### bgp full mesh





#### bgp  åå°„å™¨



https://support.huawei.com/enterprise/zh/doc/EDOC1000178149/c3e1df01



ospf

https://support.huawei.com/enterprise/zh/doc/EDOC1100082073





æ°´å¹³åˆ†å‰²





50:00





```
AR2240
```







### ibgp+ebgp



![image-20220520151243039](kubernetes-network.assets/image-20220520151243039.png)



####  é…ç½®ip

```
## AR1
sys
sysname AR1
int g 0/0/0
ip a 10.1.12.1 24


int l0
ip a 1.1.1.1 32


## AR2
sys
sysname AR2
int g 0/0/0
ip a 10.1.12.2 24


int l0
ip a 2.2.2.2 32



int g 0/0/1
ip a 10.1.23.1 24




## AR3
sys
sysname AR3
int g 0/0/0
ip a 10.1.23.2 24


int l0
ip a 3.3.3.3  32



int g 0/0/1
ip a 10.1.34.1  24



## AR4
sys
sysname AR4
int g 0/0/0
ip a 10.1.34.2 24

```





####  é…ç½®ospf





```
AR1

sys
ospf 1 router-id  1.1.1.1
a 0.0.0.0
network 1.1.1.1 0.0.0.0      ## å®£å‘Š 1.1.1.1/32
network 10.1.12.0 0.0.0.255  ## å®£å‘Š 10.1.12.0/24




AR2

sys
ospf 1 router-id 2.2.2.2
a 0.0.0.0
network 2.2.2.2 0.0.0.0
network 10.1.12.0 0.0.0.255
network 10.1.23.0 0.0.0.255





AR3

sys
ospf 1 router-id  3.3.3.3
a 0.0.0.0
network 10.1.23.0 0.0.0.255
network 3.3.3.3   0.0.0.0


AR1 AR2 AR3 æŸ¥çœ‹ospf çŠ¶æ€

dis ospf peer  brief 


```





#### AR1 AR2 IBGP



```
# AR1

sys
bgp 123

router-id 1.1.1.1
peer 3.3.3.3 as-number 123
peer 3.3.3.3 connect-interface l0




# AR3

sys
bgp 123

router-id 3.3.3.3
peer 1.1.1.1 as-number 123
peer 1.1.1.1 connect-interface l0



# AR1 AR3 

dis bgp peer




# æµ‹è¯•bgp å®£å‘Šè·¯ç”±
AR3 

ç¯å›å£1 è®¾ç½®ip
int l1
ip a 66.66.66.66 32

é€šè¿‡bgp å®£å‘Šè·¯ç”±

bgp 123
network 66.66.66.66 255.255.255.255

æŸ¥çœ‹bgp è·¯ç”±æ¡ç›®

dis bgp routing-table


AR1 æŸ¥çœ‹æ˜¯å¦æœ‰å¯¹åº”è·¯ç”±æ¡ç›®
dis bgp routing-table

```







```
dis ip routing-table
```





```
è·¯ç”±æ¡ç›®å¼•å…¥???
```





#### AR3 AR4 EBGP



```
# AR3
sys
bgp 123
peer  10.1.34.2 as-number 400





# AR4


sys
bgp 400
router-id 10.1.34.2
peer  10.1.34.1 as-number 123




dis bgp peer

```







```
brid å‘½ä»¤å
show route
```







## 19 2020-05-20-calico-pass



calico bgp full mesh



```
calicoctl get ippool -o wide
ipipmode   never
vxlanmode  never





ï¼ˆåœ¨ä¸€ä¸ªè‡ªæ²»ç³»ç»Ÿæ‰å­˜åœ¨æ°´å¹³åˆ†å‰²ï¼‰
ç”±äºæ°´å¹³åˆ†å‰² æ‰€ä»¥éœ€è¦full mesh

full mesh æ¡ä»¶

1 host äºŒå±‚å¯è¾¾ï¼Ÿï¼Ÿ  tcp/170 å¯è¾¾ã€‚ æ»¡è¶³  full mesh



calico-node
birdcl
show ?
show interface
show route
show route for 10.244.200.192 all



calicoctl get ippool -o wide
cidr 10.244.0.0/16  æ¯26ä¸ªæ˜¯ä¸€ä¸ªå— ï¼Ÿï¼Ÿ è·¯ç”±èšåˆï¼Ÿï¼Ÿ


name                   cidr             nat    ipipmode   vxlanmode   disabled      sleector
default-ipv4-ippool   10.244.0.0/16    true      never       never      false        all()


ç¦ç”¨

kubectl edit ippool default-ipv4-ippool
disabled: true

pod è·å–ä¸åˆ°åœ°å€


calicoctl get xx -o yaml xx.yaml
calicoctl path xx


æŸ¥çœ‹bird é…ç½®æ–‡ä»¶

/etc/calico/confd/config/bird.cfg
```





## 20 2020-05-22-calico-pass





https://projectcalico.docs.tigera.io/reference/vpp/uplink-configuration





è·¯ç”±åå°„å™¨











```
è·¯ç”±åå°„å™¨ä¹‹é—´å½¢æˆ full mesh

AR2 -> bgp rr

rr æŒ‡å®š  è·¯ç”±åå°„å™¨client

br2

bgp 123
peer 1.1.1.1 reflect-client
peer 3.3.3.3 reflect-client


ar3
å®£å‘Šbgp è·¯ç”±

ar1
å®£å‘Šbgp è·¯ç”±


è¿­ä»£æŸ¥è¯¢ 

è·¯ç”±è½¬å‘è·¯å¾„ 



```







```
ensp AR2240
```



è·¯ç”±åå°„å™¨





https://support.huawei.com/enterprise/zh/doc/EDOC1100058307/a2762e21/configguring-a-bgp-route-reflector



https://www.cisco.com/c/zh_cn/support/docs/ip/border-gateway-protocol-bgp/113419-ipv6-bgp-rr-00.pdf















https://projectcalico.docs.tigera.io/reference/architecture/design/l2-interconnect-fabric



https://projectcalico.docs.tigera.io/reference/architecture/design/l3-interconnect-fabric









### calico-router reflector



https://projectcalico.docs.tigera.io/getting-started/kubernetes/hardway/configure-bgp-peering





https://projectcalico.docs.tigera.io/getting-started/kubernetes/hardway/configure-bgp-peering







```
å…³é—­ ipipmode

full-mesh



peer address
peer type 
rr æŒ‡å®š rr çš„å®¢æˆ·ç«¯

node-to-node mesh  é…ç½®ä¸º node specific



calicoctl node status





```





##### 1  é…ç½®å¤–éƒ¨è·¯ç”±åå°„å™¨



https://projectcalico.docs.tigera.io/networking/bgp#configure-a-global-bgp-peer







##### 2 è®²æŸä¸ªnode ä½œä¸ºè·¯ç”±åå°„å™¨



https://projectcalico.docs.tigera.io/networking/bgp#configure-a-node-to-act-as-a-route-reflector









```
 ç»™èŠ‚ç‚¹æ‰“æ ‡ç­¾ 
 è®¾ç½® routeReflectorClusterID	

 https://projectcalico.docs.tigera.io/reference/resources/node
 
 é…ç½® peer Kind: BGPeer
 
 name peer-to-rrs   é…ç½®å¤šä¸ªrr
 
 
 
 
 è·¯ç”±åå°„å™¨å®¢æˆ·ç«¯ åˆ° è·¯ç”±åå°„å™¨ peer çš„å»ºç«‹
calicoctl apply -f - <<EOF
kind: BGPPeer
apiVersion: projectcalico.org/v3
metadata:
  name: peer-to-rrs
spec:
  nodeSelector: "!has(calico-route-reflector)"   æ™®é€šçš„IBGPå¯¹ç­‰ä½“ érr
  peerSelector: has(calico-route-reflector)      è·¯ç”±åå°„å™¨
EOF                                              ä¸¤è€…ä¹‹é—´å»ºç«‹ peer

```







```
1 å…³é—­ipipmode å…³é—­vxlanmode



rr client è§’åº¦
calicoctl node status
global


rr è§’åº¦
calicoctl node status
peer type
node specific
```





full-mesh-2-bgp-rr







```
ç»„æ’­åœ°å€

routeReflectorClusterID: 224.0.0.1 
æ‰€æœ‰ä¸»æœºçš„åœ°å€ï¼ˆåŒ…æ‹¬æ‰€æœ‰è·¯ç”±å™¨åœ°å€ï¼‰
```







## 21 2022-05-23-calico







https://projectcalico.docs.tigera.io/reference/architecture/design/l2-interconnect-fabric





https://projectcalico.docs.tigera.io/reference/architecture/design/l3-interconnect-fabric









```
The Downward Default model
åªé€šå‘Šé»˜è®¤è·¯ç”±
```







```
AR2240
```



1:21





## 22 2022-05-25-calico-pass





```
vmware e1000
vmxnet3 dpdk
```







https://projectcalico.docs.tigera.io/getting-started/kubernetes/vpp/







https://projectcalico.docs.tigera.io/getting-started/kubernetes/vpp/getting-started











ubuntu 5.11











```
vpp dataplane interface

```







```
ethtool eht0

speed 1000->10

ip tuntap list
eht0 : tap å˜æˆtapè®¾å¤‡äº†



```











https://slideplayer.com/slide/17437344





![image-20220530103932295](kubernetes-network.assets/image-20220530103932295.png)







https://suhu0426.github.io/Web/Presentation/20150203/index.html











https://projectcalico.docs.tigera.io/reference/vpp/host-network









https://projectcalico.docs.tigera.io/maintenance/troubleshoot/vpp







```
calivppctl vppctl node
show interface


ä¸¤ä¸ªeth0 mac ä¸€æ · å¹¶ä¸å†²çª å› ä¸ºä¸åœ¨ä¸€ä¸ªåè®®æ ˆä¸­

show ipip tunnel


pod eth0 ç‚¹å¯¹ç‚¹ tun è®¾å¤‡

route -n





pcap trace rx tx 30000 intfc any

pacp trace off



ipip  always + vpp
vxlan always + vpp


show vxlan tunnel


vpp å»ç¨‹ å›ç¨‹



å®¿ä¸»æœº  eth0 æ•°æ®åŒ…æµå‘

eth0-> vpp tap0   -> å¤–éƒ¨


show tap
```





## 23-2022-05-27-calico-pass





calico ipam



```
whereabouts
```







https://projectcalico.docs.tigera.io/networking/get-started-ip-addresses





```
/etc/cni/net.d/10-calico.conflist

ipam
type calico-ipam


localhost???
```







```
ippool 

å›ºå®šip åœ°å€ 


calico get ippool -i yaml



æŒ‡å®šå›ºå®šIP
```









https://projectcalico.docs.tigera.io/networking/use-specific-ip






https://cloud.ibm.com/docs/openshift?topic=openshift-ts-app-container-start&locale=es



```
calicoctl ipam check
```







## 24-2022-05-29-flannel

